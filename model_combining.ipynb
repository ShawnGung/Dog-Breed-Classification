{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from reData import *\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as utils\n",
    "import torchvision.models as models\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载特征数据 / 定义全连接网络融合特征\n",
    "\n",
    "- Net为融合特征的简单的一层全连接\n",
    "- 把特征数据从四维转成两维来输出到Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/dog_breed'        \n",
    "label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n",
    "input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Net,self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=4768, out_features=120, bias=True),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(in_features=300, out_features=120, bias=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "EPOCH = 200\n",
    "batch_size = 64\n",
    "\n",
    "train_pt = torch.load('train_r152i3.pt')\n",
    "valid_pt = torch.load('valid_r152i3.pt')\n",
    "input_pt = torch.load('input_r152i3.pt')\n",
    "test_pt = torch.load('test_r152i3.pt')\n",
    "\n",
    "#reshape [b,c,1,1] to [b,c]\n",
    "for i in range(2):\n",
    "    train_pt[0] = train_pt[0].view(train_pt[0].shape[0],train_pt[0].shape[1])\n",
    "    valid_pt[0] = valid_pt[0].view(valid_pt[0].shape[0],valid_pt[0].shape[1])\n",
    "    input_pt[0] = input_pt[0].view(input_pt[0].shape[0],input_pt[0].shape[1])\n",
    "    test_pt[0] = test_pt[0].view(test_pt[0].shape[0],test_pt[0].shape[1])\n",
    "    \n",
    "\n",
    "train_dl = DataLoader(utils.TensorDataset(train_pt[0],train_pt[1]), batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(utils.TensorDataset(valid_pt[0],valid_pt[1]), batch_size=batch_size,shuffle=True)\n",
    "train_valid_dl = DataLoader(utils.TensorDataset(input_pt[0],input_pt[1]), batch_size=batch_size,shuffle=True)\n",
    "test_dl = DataLoader(utils.TensorDataset(test_pt[0],test_pt[1]), batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,EPOCH,loss_func,optimizer,train_valid_dl,train_dl,valid_dl,use_valid = True):\n",
    "    def evaluate_lossAndAcc(data_iter, net):\n",
    "        l_sum,acc = 0.0, 0.0\n",
    "        for X, y in data_iter:\n",
    "            #cuda\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            y_pre = net(X)\n",
    "            #calculate acc\n",
    "            pred_y = torch.max(y_pre, 1)[1].cpu().data.numpy()\n",
    "            accuracy = float((pred_y == y.data.cpu().numpy()).astype(int).sum()) / float(y.size(0))\n",
    "            acc += accuracy\n",
    "            #calculate sum\n",
    "            l_sum += loss_func(y_pre, y).data.cpu().numpy()\n",
    "        return l_sum / len(data_iter), acc / len(data_iter)\n",
    "\n",
    "    if use_valid:\n",
    "        data_iter = train_dl\n",
    "    else:\n",
    "        data_iter = train_valid_dl\n",
    "        \n",
    "    for epoch in range(1,EPOCH+1):\n",
    "        start = time.time()\n",
    "        train_lsum, n= 0.0, 0\n",
    "        for step,(X,y) in enumerate(data_iter):\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            output = net(X)\n",
    "            loss = loss_func(output,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_lsum += loss.cpu().data.numpy()\n",
    "            n += float(y.size(0))\n",
    "        if epoch % 5 == 0:\n",
    "            train_loss,train_acc = evaluate_lossAndAcc(train_dl, net)\n",
    "            if use_valid:\n",
    "                valid_loss,valid_acc = evaluate_lossAndAcc(valid_dl, net)\n",
    "                print(\"epoch %d | train loss : %.4f | valid loss : %.4f | train_acc : %.4f | valid_acc : %.4f | time :%.4f sec\" \n",
    "                           % (epoch, train_loss, valid_loss,train_acc,valid_acc,(time.time() - start)))\n",
    "            else:\n",
    "                 print(\"epoch %d | train loss : %.4f | train_acc : %.4f | time :%.4f sec\" \n",
    "                           % (epoch, train_loss,train_acc,(time.time() - start)))\n",
    "#         else:\n",
    "#             print(\"epoch %d | train loss : %.4f | time :%.4f sec\" \n",
    "#                        % (epoch, train_lsum / n,(time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 提取各种深度网络的feature层\n",
    "\n",
    "- 提取VGG19,ResNet50,ResNet152,Desnet162的features层的输出\n",
    "- 通过average pooling来调整输出为(batchsize , channel, 1 , 1)\n",
    "- 通过ConcatNet 来合并所有输出\n",
    "\n",
    "*这里只选择了三种网络vgg19 / resnet152 / desnet162 进行合并*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | train loss : 0.8753 | valid loss : 0.9557 | train_acc : 0.9168 | valid_acc : 0.8815 | time :0.4157 sec\n",
      "epoch 10 | train loss : 0.4923 | valid loss : 0.5814 | train_acc : 0.9380 | valid_acc : 0.8932 | time :1.5548 sec\n",
      "epoch 15 | train loss : 0.3722 | valid loss : 0.4645 | train_acc : 0.9461 | valid_acc : 0.8971 | time :0.3535 sec\n",
      "epoch 20 | train loss : 0.3102 | valid loss : 0.4431 | train_acc : 0.9511 | valid_acc : 0.8867 | time :0.8151 sec\n",
      "epoch 25 | train loss : 0.2710 | valid loss : 0.4020 | train_acc : 0.9560 | valid_acc : 0.8880 | time :0.6144 sec\n",
      "epoch 30 | train loss : 0.2428 | valid loss : 0.3784 | train_acc : 0.9603 | valid_acc : 0.8867 | time :1.5378 sec\n",
      "epoch 35 | train loss : 0.2213 | valid loss : 0.3533 | train_acc : 0.9641 | valid_acc : 0.9010 | time :0.3081 sec\n",
      "epoch 40 | train loss : 0.2033 | valid loss : 0.3389 | train_acc : 0.9670 | valid_acc : 0.9010 | time :0.3081 sec\n",
      "epoch 45 | train loss : 0.1884 | valid loss : 0.3434 | train_acc : 0.9698 | valid_acc : 0.8971 | time :0.3146 sec\n",
      "epoch 50 | train loss : 0.1762 | valid loss : 0.3516 | train_acc : 0.9723 | valid_acc : 0.8971 | time :0.3885 sec\n",
      "epoch 55 | train loss : 0.1656 | valid loss : 0.3196 | train_acc : 0.9742 | valid_acc : 0.9036 | time :0.7152 sec\n",
      "epoch 60 | train loss : 0.1562 | valid loss : 0.3144 | train_acc : 0.9763 | valid_acc : 0.9036 | time :1.4714 sec\n",
      "epoch 65 | train loss : 0.1477 | valid loss : 0.3222 | train_acc : 0.9782 | valid_acc : 0.8984 | time :0.3500 sec\n",
      "epoch 70 | train loss : 0.1405 | valid loss : 0.3129 | train_acc : 0.9794 | valid_acc : 0.9076 | time :0.3506 sec\n",
      "epoch 75 | train loss : 0.1337 | valid loss : 0.3226 | train_acc : 0.9808 | valid_acc : 0.8971 | time :0.4349 sec\n",
      "epoch 80 | train loss : 0.1280 | valid loss : 0.3219 | train_acc : 0.9824 | valid_acc : 0.8958 | time :1.5441 sec\n",
      "epoch 85 | train loss : 0.1218 | valid loss : 0.3030 | train_acc : 0.9837 | valid_acc : 0.9036 | time :0.4750 sec\n",
      "epoch 90 | train loss : 0.1166 | valid loss : 0.3260 | train_acc : 0.9849 | valid_acc : 0.8971 | time :0.3510 sec\n",
      "epoch 95 | train loss : 0.1118 | valid loss : 0.3025 | train_acc : 0.9861 | valid_acc : 0.8984 | time :0.4335 sec\n",
      "epoch 100 | train loss : 0.1074 | valid loss : 0.3153 | train_acc : 0.9870 | valid_acc : 0.8958 | time :1.2307 sec\n",
      "epoch 105 | train loss : 0.1034 | valid loss : 0.2890 | train_acc : 0.9883 | valid_acc : 0.9062 | time :0.8255 sec\n",
      "epoch 110 | train loss : 0.0995 | valid loss : 0.3091 | train_acc : 0.9887 | valid_acc : 0.8971 | time :0.3190 sec\n",
      "epoch 115 | train loss : 0.0961 | valid loss : 0.3027 | train_acc : 0.9892 | valid_acc : 0.9010 | time :0.3112 sec\n",
      "epoch 120 | train loss : 0.0928 | valid loss : 0.2993 | train_acc : 0.9899 | valid_acc : 0.9036 | time :0.3002 sec\n",
      "epoch 125 | train loss : 0.0895 | valid loss : 0.3126 | train_acc : 0.9904 | valid_acc : 0.8971 | time :0.3187 sec\n",
      "epoch 130 | train loss : 0.0866 | valid loss : 0.2933 | train_acc : 0.9910 | valid_acc : 0.9102 | time :0.3133 sec\n",
      "epoch 135 | train loss : 0.0839 | valid loss : 0.2945 | train_acc : 0.9919 | valid_acc : 0.9062 | time :0.3109 sec\n",
      "epoch 140 | train loss : 0.0814 | valid loss : 0.2952 | train_acc : 0.9921 | valid_acc : 0.9062 | time :0.3137 sec\n",
      "epoch 145 | train loss : 0.0787 | valid loss : 0.2877 | train_acc : 0.9929 | valid_acc : 0.9102 | time :0.3134 sec\n",
      "epoch 150 | train loss : 0.0765 | valid loss : 0.3431 | train_acc : 0.9935 | valid_acc : 0.8906 | time :0.3024 sec\n",
      "epoch 155 | train loss : 0.0744 | valid loss : 0.2933 | train_acc : 0.9939 | valid_acc : 0.9102 | time :0.3145 sec\n",
      "epoch 160 | train loss : 0.0722 | valid loss : 0.3027 | train_acc : 0.9941 | valid_acc : 0.9062 | time :0.3844 sec\n",
      "epoch 165 | train loss : 0.0702 | valid loss : 0.3301 | train_acc : 0.9948 | valid_acc : 0.9010 | time :1.1393 sec\n",
      "epoch 170 | train loss : 0.0684 | valid loss : 0.2876 | train_acc : 0.9951 | valid_acc : 0.9089 | time :0.4238 sec\n",
      "epoch 175 | train loss : 0.0668 | valid loss : 0.2924 | train_acc : 0.9956 | valid_acc : 0.9076 | time :1.3254 sec\n",
      "epoch 180 | train loss : 0.0649 | valid loss : 0.2942 | train_acc : 0.9960 | valid_acc : 0.9010 | time :0.3463 sec\n",
      "epoch 185 | train loss : 0.0633 | valid loss : 0.2983 | train_acc : 0.9962 | valid_acc : 0.9010 | time :1.2353 sec\n",
      "epoch 190 | train loss : 0.0617 | valid loss : 0.2905 | train_acc : 0.9964 | valid_acc : 0.9062 | time :0.4430 sec\n",
      "epoch 195 | train loss : 0.0604 | valid loss : 0.2872 | train_acc : 0.9967 | valid_acc : 0.9089 | time :1.3965 sec\n",
      "epoch 200 | train loss : 0.0589 | valid loss : 0.3309 | train_acc : 0.9969 | valid_acc : 0.8893 | time :0.3534 sec\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "net = Net().cuda()\n",
    "optimizer = torch.optim.SGD(params=net.parameters(), lr=1e-3,momentum = 0.7,weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adam(params=net.parameters(),lr=1e-3,weight_decay=1e-3)\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "train(net,EPOCH,loss_func,optimizer,train_valid_dl,train_dl,valid_dl,use_valid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 | train loss : 0.3580 | train_acc : 0.9426 | time :0.9197 sec\n",
      "epoch 10 | train loss : 0.2380 | train_acc : 0.9579 | time :0.7564 sec\n",
      "epoch 15 | train loss : 0.1859 | train_acc : 0.9673 | time :1.1891 sec\n",
      "epoch 20 | train loss : 0.1545 | train_acc : 0.9750 | time :0.3070 sec\n",
      "epoch 25 | train loss : 0.1321 | train_acc : 0.9796 | time :0.3037 sec\n",
      "epoch 30 | train loss : 0.1163 | train_acc : 0.9828 | time :0.3148 sec\n",
      "epoch 35 | train loss : 0.1025 | train_acc : 0.9858 | time :0.3173 sec\n",
      "epoch 40 | train loss : 0.0922 | train_acc : 0.9889 | time :0.3167 sec\n",
      "epoch 45 | train loss : 0.0835 | train_acc : 0.9902 | time :0.3159 sec\n",
      "epoch 50 | train loss : 0.0761 | train_acc : 0.9931 | time :0.3161 sec\n",
      "epoch 55 | train loss : 0.0701 | train_acc : 0.9943 | time :0.3284 sec\n",
      "epoch 60 | train loss : 0.0645 | train_acc : 0.9956 | time :0.3026 sec\n",
      "epoch 65 | train loss : 0.0602 | train_acc : 0.9965 | time :0.3027 sec\n",
      "epoch 70 | train loss : 0.0560 | train_acc : 0.9966 | time :0.3348 sec\n",
      "epoch 75 | train loss : 0.0529 | train_acc : 0.9973 | time :0.3182 sec\n",
      "epoch 80 | train loss : 0.0496 | train_acc : 0.9983 | time :0.3178 sec\n",
      "epoch 85 | train loss : 0.0467 | train_acc : 0.9988 | time :0.3183 sec\n",
      "epoch 90 | train loss : 0.0446 | train_acc : 0.9988 | time :0.3228 sec\n",
      "epoch 95 | train loss : 0.0421 | train_acc : 0.9993 | time :0.3210 sec\n",
      "epoch 100 | train loss : 0.0402 | train_acc : 0.9993 | time :0.3220 sec\n",
      "epoch 105 | train loss : 0.0383 | train_acc : 0.9993 | time :0.3185 sec\n",
      "epoch 110 | train loss : 0.0367 | train_acc : 0.9993 | time :0.3068 sec\n",
      "epoch 115 | train loss : 0.0352 | train_acc : 0.9994 | time :0.3222 sec\n",
      "epoch 120 | train loss : 0.0338 | train_acc : 0.9995 | time :0.3034 sec\n",
      "epoch 125 | train loss : 0.0326 | train_acc : 0.9994 | time :0.3131 sec\n",
      "epoch 130 | train loss : 0.0315 | train_acc : 0.9994 | time :0.3183 sec\n",
      "epoch 135 | train loss : 0.0304 | train_acc : 0.9994 | time :0.3179 sec\n",
      "epoch 140 | train loss : 0.0293 | train_acc : 0.9995 | time :0.2993 sec\n",
      "epoch 145 | train loss : 0.0284 | train_acc : 0.9994 | time :0.3152 sec\n",
      "epoch 150 | train loss : 0.0275 | train_acc : 0.9995 | time :0.3215 sec\n",
      "epoch 155 | train loss : 0.0267 | train_acc : 0.9994 | time :0.3309 sec\n",
      "epoch 160 | train loss : 0.0259 | train_acc : 0.9994 | time :1.1725 sec\n",
      "epoch 165 | train loss : 0.0252 | train_acc : 0.9994 | time :0.6718 sec\n",
      "epoch 170 | train loss : 0.0245 | train_acc : 0.9995 | time :0.8218 sec\n",
      "epoch 175 | train loss : 0.0238 | train_acc : 0.9995 | time :0.7586 sec\n",
      "epoch 180 | train loss : 0.0233 | train_acc : 0.9995 | time :1.4505 sec\n",
      "epoch 185 | train loss : 0.0227 | train_acc : 0.9995 | time :0.3711 sec\n",
      "epoch 190 | train loss : 0.0221 | train_acc : 0.9994 | time :1.2531 sec\n",
      "epoch 195 | train loss : 0.0216 | train_acc : 0.9994 | time :0.3386 sec\n",
      "epoch 200 | train loss : 0.0211 | train_acc : 0.9994 | time :1.0157 sec\n"
     ]
    }
   ],
   "source": [
    "#all data (train + valid)\n",
    "net = Net().cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(params=net.parameters(), lr=1e-4,weight_decay=1e-5)\n",
    "optimizer = torch.optim.SGD(params=net.parameters(), lr=1e-3,momentum = 0.9,weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adam(params=net.parameters(),lr=1e-4,weight_decay=1e-5)\n",
    "train(net,EPOCH,loss_func,optimizer,train_valid_dl,train_dl,valid_dl,use_valid = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval()\n",
    "def prediction(data_iter,net):\n",
    "    preds = []\n",
    "    for X, y in data_iter:\n",
    "        X = X.cuda()\n",
    "        output = net(X)\n",
    "        softmax = nn.Softmax(1)# softmax on each row\n",
    "        pred = softmax(output).data.cpu().numpy()\n",
    "        preds.extend(pred)\n",
    "    return preds\n",
    "preds = np.array(prediction(test_dl,net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n",
    "for i, c in enumerate(df_pred.columns[1:]):\n",
    "    df_pred[c] = preds[:,i]\n",
    "df_pred.to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
